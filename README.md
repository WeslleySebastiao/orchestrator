# A2A Orchestrator

Sistema de orquestraÃ§Ã£o multi-agente com LangGraph. Um chat que classifica mensagens, coleta informaÃ§Ãµes e despacha para agentes externos via API â€” tudo com LLM real por trÃ¡s.

## VisÃ£o Geral

O A2A funciona como um orquestrador central: recebe mensagens do usuÃ¡rio, entende o que ele precisa, coleta as informaÃ§Ãµes necessÃ¡rias ao longo da conversa e, quando tudo estÃ¡ pronto, chama o agente especializado via HTTP.

```
UsuÃ¡rio
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 A2A Server (LangGraph)              â”‚
â”‚                                                    â”‚
â”‚  intake â†’ classification â”€â”¬â”€ small_talk â”€â”         â”‚
â”‚           (ğŸ§  LLM)       â”œâ”€ clarify    â”€â”¤         â”‚
â”‚                           â”œâ”€ self_serve â”€â”¤â†’ synthesis (ğŸ§  LLM)
â”‚                           â””â”€ dispatch   â”€â”˜         â”‚
â”‚                                 â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                            â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                            â”‚    API    â”‚
                            â”‚  Externa  â”‚
                            â”‚ (Agentes) â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## PrincÃ­pio Arquitetural

Todos os nÃ³s se comunicam via **dados estruturados (JSON)**. O **Synthesis Ã© o Ãºnico nÃ³ que gera linguagem natural**, aplicando o tom de voz configurado.

| NÃ³ | Usa LLM? | O que produz |
|---|---|---|
| `intake` | âŒ | Registra mensagem no histÃ³rico |
| `classification` | âœ… | `Classification` JSON â€” mode, intent, confidence, slots extraÃ­dos |
| `small_talk` | âŒ | `NodeResult` JSON â€” tipo de conversa, contexto |
| `clarify` | âŒ | `NodeResult` JSON â€” slots pendentes, pergunta a fazer |
| `self_serve` | âŒ | `NodeResult` JSON â€” resultado de consulta interna |
| `dispatch` | âŒ | `NodeResult` JSON â€” resposta da API externa ou erro |
| `synthesis` | âœ… | **Texto natural** com tom de voz â†’ resposta ao usuÃ¡rio |

**Chamadas LLM por turno: exatamente 2** (classification + synthesis). Sempre.

---

## Estrutura do Projeto

```
a2a-orchestrator/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # LLM factory, VOICE_TONE, agent registry
â”‚   â”œâ”€â”€ graph.py               # DefiniÃ§Ã£o do grafo LangGraph
â”‚   â”œâ”€â”€ schemas.py             # GraphState, Classification, NodeResult, API contracts
â”‚   â”œâ”€â”€ server.py              # FastAPI endpoints
â”‚   â”œâ”€â”€ session.py             # Gerenciador de sessÃµes in-memory
â”‚   â””â”€â”€ nodes/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ intake.py          # Registra mensagem (sem LLM)
â”‚       â”œâ”€â”€ classification.py  # LLM classifica â†’ JSON estruturado
â”‚       â”œâ”€â”€ small_talk.py      # Conversa geral â†’ NodeResult (sem LLM)
â”‚       â”œâ”€â”€ clarify.py         # Coleta de slots â†’ NodeResult (sem LLM)
â”‚       â”œâ”€â”€ self_serve.py      # ResoluÃ§Ã£o interna â†’ NodeResult (sem LLM)
â”‚       â”œâ”€â”€ dispatch.py        # Chama API externa â†’ NodeResult (sem LLM)
â”‚       â””â”€â”€ synthesis.py       # NodeResult â†’ linguagem natural (LLM)
â”œâ”€â”€ main.py                    # Entrypoint do server
â”œâ”€â”€ cli.py                     # Cliente CLI para testes
â”œâ”€â”€ mock_agents_api.py         # Mock da API de agentes
â”œâ”€â”€ test_dispatch.py           # Suite de testes automatizados
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## Quick Start

### 1. Instalar dependÃªncias

```bash
git clone <repo-url>
cd a2a-orchestrator
pip install -r requirements.txt
```

### 2. Configurar variÃ¡veis de ambiente

```bash
cp .env.example .env
```

Edite o `.env` com sua chave:

```env
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini
```

### 3. Subir o mock de agentes

```bash
python mock_agents_api.py
# Roda na porta 8001
```

### 4. Subir o orquestrador

```bash
python main.py
# Roda na porta 8000
```

### 5. Testar

```bash
# Via CLI interativo
python cli.py

# Ou via curl
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Oi!"}'
```

---

## ConfiguraÃ§Ã£o

### VariÃ¡veis de ambiente

| VariÃ¡vel | DescriÃ§Ã£o | Default |
|---|---|---|
| `OPENAI_API_KEY` | Chave da API OpenAI | â€” (obrigatÃ³rio) |
| `OPENAI_MODEL` | Modelo a usar | `gpt-4o-mini` |
| `VOICE_TONE` | System prompt do Synthesis (tom de voz) | Lia, assistente da Klabin |
| `AGENTS_API_BASE_URL` | URL da API de agentes | `http://localhost:8001` |
| `AGENTS_API_KEY` | Bearer token para API de agentes | â€” (opcional) |
| `HOST` | Host do server | `0.0.0.0` |
| `PORT` | Porta do server | `8000` |

### Tom de Voz

O tom de voz Ã© controlado pela variÃ¡vel `VOICE_TONE`. Ã‰ o system prompt que o Synthesis usa para **toda** resposta. Para mudar a personalidade, edite apenas essa variÃ¡vel:

```env
VOICE_TONE=VocÃª Ã© o Max, assistente tÃ©cnico. Seja direto, use termos tÃ©cnicos quando necessÃ¡rio, sem emojis.
```

Nenhum cÃ³digo precisa mudar.

### Trocar o LLM Provider

Para usar Azure OpenAI, edite `app/config.py`:

```python
from langchain_openai import AzureChatOpenAI

def get_llm() -> AzureChatOpenAI:
    return AzureChatOpenAI(
        azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT"),
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
        api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-10-21"),
        temperature=0,
    )
```

Para Anthropic:

```python
from langchain_anthropic import ChatAnthropic

def get_llm() -> ChatAnthropic:
    return ChatAnthropic(
        model=os.getenv("ANTHROPIC_MODEL", "claude-sonnet-4-20250514"),
        api_key=os.getenv("ANTHROPIC_API_KEY"),
        temperature=0,
    )
```

---

## API Endpoints

### `POST /chat`

Endpoint principal. Recebe mensagem, executa o grafo, retorna resposta.

**Request:**

```json
{
  "session_id": "uuid-opcional",
  "message": "Preciso de um lembrete"
}
```

Se `session_id` for omitido, cria uma nova sessÃ£o.

**Response:**

```json
{
  "session_id": "abc-123",
  "response": "Claro! O que vocÃª quer lembrar e pra que horas?",
  "classification": {
    "mode": "clarify",
    "intent": "lembrete",
    "confidence": 0.92,
    "missing_slots": ["descricao", "horario"],
    "question_to_ask": "ask_descricao_horario",
    "candidate_agents": ["agent-lembrete"],
    "extracted_slots": {}
  },
  "node_result": {
    "source_node": "clarify",
    "intent": "lembrete",
    "status": "pending",
    "data": {
      "agent_name": "Agente Lembrete",
      "agent_id": "agent-lembrete"
    },
    "slots_collected": {},
    "missing_slots": ["descricao", "horario"],
    "question_to_ask": "ask_descricao_horario",
    "error_message": null
  },
  "agent_result": null,
  "debug": {
    "slots": {},
    "current_intent": "lembrete",
    "message_count": 2,
    "node_path": ["intake", "classification", "clarify", "synthesis"]
  }
}
```

### `GET /sessions`

Lista sessÃµes ativas.

### `DELETE /sessions/{session_id}`

Remove uma sessÃ£o.

### `GET /health`

Health check.

---

## Fluxo de ClassificaÃ§Ã£o

A LLM recebe a mensagem do usuÃ¡rio + contexto (histÃ³rico, slots coletados, intent acumulada, agentes disponÃ­veis) e retorna um JSON com:

| Campo | DescriÃ§Ã£o |
|---|---|
| `mode` | Rota: `small_talk`, `clarify`, `self_serve`, `dispatch` |
| `intent` | Intent identificada (ex: `clima`, `lembrete`, `traduzir`) |
| `confidence` | Score 0-1 |
| `missing_slots` | Slots que ainda faltam |
| `question_to_ask` | IndicaÃ§Ã£o do que perguntar |
| `candidate_agents` | IDs dos agentes candidatos |
| `extracted_slots` | Slots extraÃ­dos da mensagem atual |

### Regras de roteamento

- **`small_talk`** â€” Default. Tudo que nÃ£o corresponde a um agente. Conversa livre.
- **`clarify`** â€” Intent identificada mas faltam slots. ObrigatÃ³rio ter `intent` + `missing_slots`.
- **`self_serve`** â€” Todos os slots preenchidos, agente marcado como `self_serve=True`.
- **`dispatch`** â€” Todos os slots preenchidos, requer execuÃ§Ã£o externa.

### Hard guards (proteÃ§Ã£o no cÃ³digo)

Mesmo que a LLM erre, o cÃ³digo forÃ§a:

- `clarify` sem `intent` ou sem `missing_slots` â†’ vira `small_talk`
- `dispatch` / `self_serve` sem `intent` â†’ vira `small_talk`
- Erro de parse no JSON â†’ vira `small_talk`

Isso garante que o sistema **nunca trava** em loops de clarify sem saÃ­da.

### ExtraÃ§Ã£o agressiva de slots

O classificador extrai **todos os slots possÃ­veis** de uma Ãºnica mensagem:

```
"Me lembra de comprar pÃ£o Ã s 18h"
â†’ extracted_slots: {"descricao": "comprar pÃ£o", "horario": "18:00"}
â†’ dispatch direto (sem clarify intermediÃ¡rio)
```

---

## Fluxo de Dados: Exemplo Completo

CenÃ¡rio: usuÃ¡rio quer um parabÃ©ns para o JoÃ£o no dia 15/03.

```
User: "Manda um parabÃ©ns pro JoÃ£o dia 15 de marÃ§o"

1. intake       â†’ registra no histÃ³rico
2. classification (LLM) â†’
   {
     mode: "dispatch",
     intent: "happy_birthday",
     extracted_slots: {"nome": "JoÃ£o", "data": "15/03"},
     missing_slots: []
   }
3. dispatch     â†’ HTTP POST /agents/agent-happy-birthday/execute
                   body: {"intent": "happy_birthday", "slots": {"nome": "JoÃ£o", "data": "15/03"}}
                â†’ NodeResult {status: "ok", data: {mensagem: "ğŸ‚ Feliz aniversÃ¡rio, JoÃ£o!"}}
4. synthesis (LLM) â†’ "Pronto! A mensagem de parabÃ©ns pro JoÃ£o foi gerada: ğŸ‚ Feliz aniversÃ¡rio, JoÃ£o!"
```

CenÃ¡rio com coleta de slots:

```
User: "Quero traduzir uma coisa"

1. classification â†’ {mode: "clarify", intent: "traduzir", missing_slots: ["texto", "idioma"]}
2. clarify       â†’ NodeResult {status: "pending", missing_slots: ["texto", "idioma"]}
3. synthesis     â†’ "O que vocÃª quer traduzir e pra qual idioma?"

User: "Hello world pro japonÃªs"

1. classification â†’ {mode: "dispatch", intent: "traduzir", extracted_slots: {"texto": "Hello world", "idioma": "japonÃªs"}}
2. dispatch      â†’ HTTP POST â†’ NodeResult {status: "ok", data: {traducao: "..."}}
3. synthesis     â†’ "Aqui estÃ¡: ..."
```

---

## Gerenciamento de Contexto

### O que persiste entre turnos

O `GraphState` mantÃ©m entre mensagens:

| Campo | DescriÃ§Ã£o | Persiste? |
|---|---|---|
| `messages` | HistÃ³rico completo (user + assistant) | âœ… Acumula |
| `slots` | Slots coletados | âœ… AtÃ© dispatch/self_serve resetar |
| `current_intent` | Intent em andamento | âœ… AtÃ© dispatch/self_serve resetar |
| `session_id` | Identificador da sessÃ£o | âœ… Sempre |
| `classification` | ClassificaÃ§Ã£o do turno | âŒ Sobrescrito a cada turno |
| `node_result` | Resultado do nÃ³ | âŒ Sobrescrito a cada turno |
| `response` | Resposta final | âŒ Sobrescrito a cada turno |

### O que cada LLM vÃª

**Classification** recebe:
- System prompt com agentes disponÃ­veis, slots coletados e intent acumulada
- Ãšltimas 10 mensagens (user + assistant) como contexto
- Mensagem atual do usuÃ¡rio

**Synthesis** recebe:
- `VOICE_TONE` como system prompt
- Ãšltimas 8 mensagens (user + assistant) para tom e continuidade
- `NodeResult` serializado como JSON
- Mensagem atual do usuÃ¡rio (para espelhar o estilo)

### SessÃµes

SessÃµes sÃ£o armazenadas in-memory (`dict` Python). Para produÃ§Ã£o, substituir `SessionManager` em `app/session.py` por Redis, PostgreSQL ou outro backend persistente.

---

## Agentes DisponÃ­veis (Mock)

O projeto inclui um mock (`mock_agents_api.py`) com 4 serviÃ§os genÃ©ricos:

| Intent | Agente | Slots | DescriÃ§Ã£o |
|---|---|---|---|
| `happy_birthday` | agent-happy-birthday | `nome`, `data` | Gera mensagem de aniversÃ¡rio |
| `clima` | agent-clima | `cidade` | Consulta previsÃ£o do tempo |
| `traduzir` | agent-traduzir | `texto`, `idioma` | Traduz texto |
| `lembrete` | agent-lembrete | `descricao`, `horario` | Cria lembrete |

Qualquer intent desconhecida recebe um fallback genÃ©rico que ecoa os parÃ¢metros.

### Contrato da API de Agentes

O dispatch chama:

```
POST /agents/{agent_id}/execute
Content-Type: application/json

{
  "intent": "happy_birthday",
  "slots": {
    "nome": "JoÃ£o",
    "data": "15/03"
  }
}
```

Resposta esperada:

```json
{
  "agent_id": "agent-happy-birthday",
  "status": "success",
  "response": "ParabÃ©ns gerado para JoÃ£o na data 15/03.",
  "data": {
    "nome": "JoÃ£o",
    "data": "15/03",
    "mensagem": "ğŸ‚ Feliz aniversÃ¡rio, JoÃ£o!"
  }
}
```

Para adicionar novos agentes:
1. Adicionar `AgentCard` no `AGENT_REGISTRY` em `app/config.py`
2. Implementar o handler na API de agentes

---

## Testes

### CLI interativo

```bash
python cli.py              # Via HTTP (server rodando)
python cli.py --direct     # Executa grafo direto (sem server)
```

O CLI mostra debug com classificaÃ§Ã£o, node_result e path. Toggle com `debug` no prompt.

### Suite automatizada

```bash
python test_dispatch.py              # Todos os cenÃ¡rios via HTTP
python test_dispatch.py -s clima     # CenÃ¡rio especÃ­fico
python test_dispatch.py --direct     # Sem server
```

CenÃ¡rios incluÃ­dos:

| CenÃ¡rio | O que testa |
|---|---|
| `holerite` | Fluxo completo: small_talk â†’ clarify â†’ clarify â†’ dispatch |
| `chamado` | Dispatch rÃ¡pido: clarify â†’ dispatch |
| `ferias` | Self-serve: clarify â†’ self_serve |
| `conversa` | 5 mensagens de small_talk variado |

### curl

```bash
# Primeira mensagem (cria sessÃ£o)
curl -s -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Oi!"}' | python -m json.tool

# Mensagem com sessÃ£o existente
curl -s -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"session_id": "SEU-SESSION-ID", "message": "Clima em Curitiba"}' | python -m json.tool
```

---

## Adicionando Novos Agentes

### 1. Registrar no config

Em `app/config.py`, adicione ao `AGENT_REGISTRY`:

```python
"meu_servico": AgentCard(
    id="agent-meu-servico",
    name="Agente Meu ServiÃ§o",
    description="DescriÃ§Ã£o clara do que faz (a LLM usa isso para classificar)",
    required_slots=["param1", "param2"],
    self_serve=False,  # True se resolver internamente
),
```

### 2. Implementar na API externa

Adicione o handler na sua API de agentes que receba `POST /agents/agent-meu-servico/execute` com o payload `{intent, slots}`.

### 3. Testar

```bash
python cli.py --direct
> Preciso do meu serviÃ§o com param1=X e param2=Y
```

O classificador vai detectar automaticamente pela descriÃ§Ã£o no registry. Se nÃ£o detectar bem, ajuste a `description` para ser mais explÃ­cita.

---

## CustomizaÃ§Ã£o

### Mudar o tom de voz

Apenas edite `VOICE_TONE` no `.env`. Exemplos:

```env
# Formal e tÃ©cnico
VOICE_TONE=VocÃª Ã© um assistente tÃ©cnico. Seja preciso, use terminologia correta, sem emojis.

# Casual e jovem
VOICE_TONE=VocÃª Ã© o LÃ©o, assistente da galera. Fala informal, usa gÃ­rias, pode usar emoji ğŸ˜

# BilÃ­ngue
VOICE_TONE=You are Lia, Klabin's virtual assistant. Respond in the same language the user writes in.
```

### Mudar o modelo

```env
OPENAI_MODEL=gpt-4o         # Mais inteligente, mais caro
OPENAI_MODEL=gpt-4o-mini    # Bom equilÃ­brio (default)
OPENAI_MODEL=gpt-3.5-turbo  # Mais barato, menos preciso
```
----

## Stack

- **[LangGraph](https://github.com/langchain-ai/langgraph)** â€” OrquestraÃ§Ã£o de grafos com estado
- **[LangChain](https://github.com/langchain-ai/langchain)** â€” AbstraÃ§Ã£o de LLMs
- **[FastAPI](https://fastapi.tiangolo.com/)** â€” API HTTP
- **[Pydantic](https://docs.pydantic.dev/)** â€” ValidaÃ§Ã£o de schemas
- **[OpenAI](https://platform.openai.com/)** â€” LLM (substituÃ­vel por Azure OpenAI ou Anthropic)